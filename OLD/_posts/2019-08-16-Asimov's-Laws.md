---
layout: default
title: "Asimovâ€™s Laws"
tags: ai
---

## Why Isaac Asimov's Laws of Robotics Don't Work

Today I want to talk about how the Isaac Asimov's Laws of Robotics dont work and they can't be used to prevent an ASI that can harm humans.

In that first sentence I already told the problem, and that is the terms 'harm' and 'humans', but first, we need to know which are the three laws of robotics and who Isaac Asimov was.

Isaac Asimov was a science-fiction writer, with many famous books like I, Robot in which he introduced the three laws:

(from Wikipedia):

The Three Laws of Robotics (often shortened to The Three Laws or known as Asimov's Laws) are a set of rules devised by the science fiction author Isaac Asimov. The rules were introduced in his 1942 short story "Runaround" (included in the 1950 collection I, Robot), although they had been foreshadowed in a few earlier stories. The Three Laws, quoted as being from the "Handbook of Robotics, 56th Edition, 2058 A.D.", are:

<h2 id="heading-two">First Law</h2>
A robot may not injure a human being or, through inaction, allow a human being to come to harm.

<h2 id="heading-two">Second Law</h2>
A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.

<h2 id="heading-two">Third Law</h2>
A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.

In the books, these laws are broken by ASI, that should be a clue that they don't work. Its true that in the books these problems are fixed, but we are talking about ASI and in the real world there will not be a way to fix it due to the high level of intelligence that we are facing.

There is a problem on the definition of the laws. They contain the words HUMAN and HARM, they are ambiguous terms that we cannot define in a language because they rely on human intuiton. If we try to define them for a computer it will result in a philisophical paradox that the ASI wont be able to handle causing it to do things that we don't want.

What about animals. Is not very ethical to kill some animals but yet others are being killed with not remorse like insects. Do some animals have priority versus another ones?

Right now whe are development thechnologies to be able to copy somebodys brain into a computer. Does that count as a human? This is an edge problem that we are not facing today but that we certainly will have in the future, just as many others that we cannot think about now because they dont exist yet.

It's a really hard problem, because we can agree that, me writing this article and you reading it we are both humans, and the computer we are reading it on is not a human, but the problem with this ambiguous terms is the edge of their domain; like, is it a 7 m/o fetus a human? is it a person who will be born in 3 years a human? what about people that they are already dead? they are humans too they are just not alive, but how do you define alive so an ASI can understand it?

People who are in a vegetative state count as alive? What about a person that just had a heart attack? They are dead but doing CPR will solve the problem in most cases, but an ASI will not try it do to the definition of alive that we gave it.

These are ethical issues that are not yet resolved and need to be before we can create ASI. We first need to solve very very hard philosophical and ethical problems that have been around since the beginning of the years.